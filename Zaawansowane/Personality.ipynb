{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import truncnorm\n",
    "import random\n",
    "from scipy.stats import t, sem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_weekly_hours(personality_type, n_samples, mean, std):\n",
    "    \"\"\"\n",
    "    Generate a sample of weekly hours (work or social activity) for a given personality type,\n",
    "    using a truncated normal distribution and rounding to the nearest 0.5.\n",
    "    \n",
    "    Args:\n",
    "    personality_type (str): The personality type.\n",
    "    n_samples (int): The number of samples to generate.\n",
    "    mean (float): The mean hours for the normal distribution.\n",
    "    std (float): The standard deviation for the normal distribution.\n",
    "    \n",
    "    Returns:\n",
    "    np.array: An array of generated weekly hours.\n",
    "    \"\"\"\n",
    "    lower_bound, upper_bound = (0, 84)\n",
    "    a, b = (lower_bound - mean) / std, (upper_bound - mean) / std\n",
    "    sample = truncnorm(a, b, loc=mean, scale=std).rvs(n_samples)\n",
    "    sample_rounded = np.round(sample * 2) / 2\n",
    "    return sample_rounded\n",
    "\n",
    "def generate_personality_hours_data(n_samples, personality_config):\n",
    "    \"\"\"\n",
    "    Generate a DataFrame with weekly hours for specified proportions and types of personalities.\n",
    "    \n",
    "    Args:\n",
    "    n_samples (int): The total number of samples to generate.\n",
    "    personality_config (dict): Configuration for each personality, including proportion, mean, and std.\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: A DataFrame with personality types and corresponding weekly hours.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    for personality, config in personality_config.items():\n",
    "        n_personality_samples = int(n_samples * config['proportion'])\n",
    "        personality_hours = generate_weekly_hours(personality, n_personality_samples, config['mean'], config['std'])\n",
    "        data.extend([(personality, hours) for hours in personality_hours])\n",
    "    \n",
    "    df = pd.DataFrame(data, columns=[\"Personality\", \"Weekly_Hours\"])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for each personality type (Type A, Type B, Type C)\n",
    "personality_config = {\n",
    "    'A': {'proportion': 0.29, 'mean': 50, 'std': 10},\n",
    "    'B': {'proportion': 0.23, 'mean': 45.5, 'std': 8},\n",
    "    'C': {'proportion': 0.48, 'mean': 47.5, 'std': 9}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Personality</th>\n",
       "      <th>Weekly_Hours</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "      <td>50.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A</td>\n",
       "      <td>33.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A</td>\n",
       "      <td>54.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A</td>\n",
       "      <td>67.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A</td>\n",
       "      <td>45.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Personality  Weekly_Hours\n",
       "0           A          50.5\n",
       "1           A          33.0\n",
       "2           A          54.5\n",
       "3           A          67.0\n",
       "4           A          45.0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate\n",
    "sample_size = random.randint(3_000_000, 5_000_000) \n",
    "generated_data = generate_personality_hours_data(sample_size, personality_config)\n",
    "generated_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To save the generated dataset as a CSV file\n",
    "file_path = r\"C:\\Users\\Admin\\OneDrive - University of Gdansk (for Students)\\Teaching\\Statystyka\\Statystyka\\Dane\\Wprowadzenie\\Osobowosc3.csv\"\n",
    "generated_data.to_csv(file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cohens_d(group1, group2):\n",
    "    \"\"\"\n",
    "    Calculate Cohen's d for effect size between two groups.\n",
    "    \"\"\"\n",
    "    # Calculate the size of samples\n",
    "    n1, n2 = len(group1), len(group2)\n",
    "    # Calculate the variance for each group\n",
    "    var1, var2 = np.var(group1, ddof=1), np.var(group2, ddof=1)\n",
    "    # Calculate the pooled standard deviation (s)\n",
    "    pooled_std = np.sqrt(((n1 - 1) * var1 + (n2 - 1) * var2) / (n1 + n2 - 2))\n",
    "    # Calculate Cohen's d\n",
    "    d = (np.mean(group1) - np.mean(group2)) / pooled_std\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_effect_sizes(data, n, k, group_column, value_column):\n",
    "    \"\"\"\n",
    "    Generate k samples of size n for each group, calculate Cohen's d for effect size between groups,\n",
    "    and create a DataFrame with these effect sizes along with the sample indices.\n",
    "\n",
    "    Parameters:\n",
    "    - data: DataFrame containing the original data.\n",
    "    - n: Size of each sample per group.\n",
    "    - k: Number of samples to generate.\n",
    "    - group_column: The column specifying the group (e.g., Type A or Type B).\n",
    "    - value_column: The column from the data on which to calculate the effect size.\n",
    "\n",
    "    Returns:\n",
    "    - A DataFrame containing Cohen's d for each of the k samples and the indices of the samples.\n",
    "    \"\"\"\n",
    "    effect_sizes = []\n",
    "    sample_indices_a = []\n",
    "    sample_indices_b = []\n",
    "    \n",
    "    for _ in range(k):\n",
    "        # Sampling with replacement within each group and keeping track of the indices\n",
    "        sample_a_indices = data[data[group_column] == 'A'].sample(n, replace=True).index\n",
    "        sample_b_indices = data[data[group_column] == 'B'].sample(n, replace=True).index\n",
    "        \n",
    "        sample_a = data.loc[sample_a_indices, value_column]\n",
    "        sample_b = data.loc[sample_b_indices, value_column]\n",
    "        \n",
    "        # Calculate Cohen's d\n",
    "        d = cohens_d(sample_a, sample_b)\n",
    "        \n",
    "        effect_sizes.append(d)\n",
    "        sample_indices_a.append(sample_a_indices.tolist())  # Store indices as a list\n",
    "        sample_indices_b.append(sample_b_indices.tolist())\n",
    "    \n",
    "    # Create a DataFrame with the collected effect sizes and indices\n",
    "    effect_size_df = pd.DataFrame({\n",
    "        'Cohen_d': effect_sizes,\n",
    "        'Sample_Indices_A': sample_indices_a,\n",
    "        'Sample_Indices_B': sample_indices_b\n",
    "    })\n",
    "    \n",
    "    return effect_size_df\n",
    "\n",
    "# Example usage\n",
    "# effect_sizes_df = sample_effect_sizes_with_tracking(df, n=50, k=100, group_column='Personality', value_column='Weekly_Hours')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To generate 100 samples of size 50 per group and calculate effect sizes:\n",
    "effect_sizes_df = sample_effect_sizes(generated_data, n=50, k=1000, group_column='Personality', value_column='Weekly_Hours')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_and_store_subsamples(effect_sizes_df, data, population_d, group_column, value_column):\n",
    "    \"\"\"\n",
    "    Identify subsamples based on Cohen's d criteria and store them in a dictionary of DataFrames.\n",
    "    \n",
    "    Parameters:\n",
    "    - effect_sizes_df: DataFrame with effect sizes and sample indices.\n",
    "    - data: Original DataFrame containing the data.\n",
    "    - population_d: Population effect size for reference.\n",
    "    - group_column, value_column: Column names for the group and the value.\n",
    "    \n",
    "    Returns:\n",
    "    - A dictionary of DataFrames for the identified subsamples.\n",
    "    \"\"\"\n",
    "    subsamples = {}\n",
    "    \n",
    "    # Identifying samples based on Cohen's d criteria\n",
    "    criteria = {\n",
    "        'close_to_population': np.abs(effect_sizes_df['Cohen_d'] - population_d).idxmin(),\n",
    "        'non_significant': np.abs(effect_sizes_df['Cohen_d']).idxmin(),\n",
    "        'reversed_effect': (effect_sizes_df['Cohen_d'] * np.sign(population_d)).idxmin(),\n",
    "        'exaggerated_effect': effect_sizes_df['Cohen_d'].idxmax() if population_d > 0 else effect_sizes_df['Cohen_d'].idxmin()\n",
    "    }\n",
    "    \n",
    "    for criterion, idx in criteria.items():\n",
    "        sample_indices_a = effect_sizes_df.loc[idx, 'Sample_Indices_A']\n",
    "        sample_indices_b = effect_sizes_df.loc[idx, 'Sample_Indices_B']\n",
    "        \n",
    "        # Extracting subsamples from the original data\n",
    "        subsample = data.loc[sample_indices_a + sample_indices_b, [group_column, value_column]]\n",
    "        \n",
    "        # Storing subsamples in the dictionary\n",
    "        subsamples[criterion] = subsample\n",
    "\n",
    "    return subsamples\n",
    "\n",
    "# Example usage\n",
    "\n",
    "subsample_dict = identify_and_store_subsamples(effect_sizes_df, generated_data, 0.4889332, 'Personality', 'Weekly_Hours')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'close_to_population': {'Statistics_Group_A': {'mean': 51.34,\n",
       "   'std_dev': 11.422069579081882,\n",
       "   'median': 49.5},\n",
       "  'Statistics_Group_B': {'mean': 46.67,\n",
       "   'std_dev': 7.205872038603982,\n",
       "   'median': 47.5},\n",
       "  'Cohens_d': 0.48902766896754457},\n",
       " 'non_significant': {'Statistics_Group_A': {'mean': 46.9,\n",
       "   'std_dev': 9.235026490796564,\n",
       "   'median': 48.25},\n",
       "  'Statistics_Group_B': {'mean': 46.85,\n",
       "   'std_dev': 8.083903128805934,\n",
       "   'median': 47.5},\n",
       "  'Cohens_d': 0.005761316872427656},\n",
       " 'reversed_effect': {'Statistics_Group_A': {'mean': 47.69,\n",
       "   'std_dev': 10.358320130135356,\n",
       "   'median': 47.0},\n",
       "  'Statistics_Group_B': {'mean': 48.67,\n",
       "   'std_dev': 7.489863898929658,\n",
       "   'median': 49.5},\n",
       "  'Cohens_d': -0.10842377897396231},\n",
       " 'exaggerated_effect': {'Statistics_Group_A': {'mean': 52.96,\n",
       "   'std_dev': 7.7793814349345345,\n",
       "   'median': 53.0},\n",
       "  'Statistics_Group_B': {'mean': 44.52,\n",
       "   'std_dev': 6.8100765676417225,\n",
       "   'median': 44.25},\n",
       "  'Cohens_d': 1.1544546651945995}}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calculate_basic_statistics_and_cohens_d(subsample_dict, group_column, value_column):\n",
    "    \"\"\"\n",
    "    Calculate basic statistics and Cohen's d for each subsample in the dictionary.\n",
    "    \n",
    "    Parameters:\n",
    "    - subsample_dict: Dictionary of DataFrames for the identified subsamples.\n",
    "    - group_column: The column specifying the group (e.g., 'Type A' or 'Type B').\n",
    "    - value_column: The column from the data on which to calculate the statistics.\n",
    "    \n",
    "    Returns:\n",
    "    - A dictionary containing basic statistics and Cohen's d for each subsample.\n",
    "    \"\"\"\n",
    "    statistics_results = {}\n",
    "    \n",
    "    for key, subsample in subsample_dict.items():\n",
    "        # Separate the groups\n",
    "        group_a = subsample[subsample[group_column] == 'A'][value_column]\n",
    "        group_b = subsample[subsample[group_column] == 'B'][value_column]\n",
    "        \n",
    "        # Calculate basic statistics\n",
    "        stats_a = {'mean': group_a.mean(), 'std_dev': group_a.std(), 'median': group_a.median()}\n",
    "        stats_b = {'mean': group_b.mean(), 'std_dev': group_b.std(), 'median': group_b.median()}\n",
    "        \n",
    "        # Calculate Cohen's d\n",
    "        d = cohens_d(group_a, group_b)\n",
    "        \n",
    "        # Store results\n",
    "        statistics_results[key] = {\n",
    "            'Statistics_Group_A': stats_a,\n",
    "            'Statistics_Group_B': stats_b,\n",
    "            'Cohens_d': d\n",
    "        }\n",
    "    \n",
    "    return statistics_results\n",
    "\n",
    "# Make sure to define the cohens_d function as before or include it within this function for completeness\n",
    "\n",
    "# Example usage:\n",
    "\n",
    "calculate_basic_statistics_and_cohens_d(subsample_dict, 'Personality', 'Weekly_Hours')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved close_to_population sample to subsample_close_to_population.csv\n",
      "Saved non_significant sample to subsample_non_significant.csv\n",
      "Saved reversed_effect sample to subsample_reversed_effect.csv\n",
      "Saved exaggerated_effect sample to subsample_exaggerated_effect.csv\n"
     ]
    }
   ],
   "source": [
    "def identify_and_save_subsamples(effect_sizes_df, data, population_d, group_column, value_column, file_prefix):\n",
    "    \"\"\"\n",
    "    Identify subsamples based on Cohen's d criteria and save them into separate files.\n",
    "    \n",
    "    Parameters:\n",
    "    - effect_sizes_df: DataFrame with effect sizes and sample indices.\n",
    "    - data: Original DataFrame containing the data.\n",
    "    - population_d: Population effect size for reference.\n",
    "    - group_column, value_column: Column names for the group and the value.\n",
    "    - file_prefix: Prefix for the output file names.\n",
    "    \"\"\"\n",
    "    # Identifying samples based on Cohen's d criteria\n",
    "    criteria = {\n",
    "        'close_to_population': np.abs(effect_sizes_df['Cohen_d'] - population_d).idxmin(),\n",
    "        'non_significant': np.abs(effect_sizes_df['Cohen_d']).idxmin(),\n",
    "        'reversed_effect': (effect_sizes_df['Cohen_d'] * np.sign(population_d)).idxmin(),\n",
    "        'exaggerated_effect': effect_sizes_df['Cohen_d'].idxmax() if population_d > 0 else effect_sizes_df['Cohen_d'].idxmin()\n",
    "    }\n",
    "    \n",
    "    for criterion, idx in criteria.items():\n",
    "        sample_indices_a = effect_sizes_df.loc[idx, 'Sample_Indices_A']\n",
    "        sample_indices_b = effect_sizes_df.loc[idx, 'Sample_Indices_B']\n",
    "        \n",
    "        # Extracting subsamples from the original data\n",
    "        subsample = data.loc[sample_indices_a + sample_indices_b, [group_column, value_column]]\n",
    "        \n",
    "        # Saving subsamples to files\n",
    "        filename = f\"{file_prefix}_{criterion}.csv\"\n",
    "        subsample.to_csv(filename, index=False)\n",
    "        print(f\"Saved {criterion} sample to {filename}\")\n",
    "\n",
    "# Example usage\n",
    "# Assuming 'effect_sizes_df' is the DataFrame output from the sample effect size tracking function,\n",
    "# 'df' is your original DataFrame, and the population effect size 'population_d' is known:\n",
    "\n",
    "# Example usage\n",
    "# Assuming 'effect_sizes_df' is the DataFrame output from the sample effect size tracking function,\n",
    "# 'df' is your original DataFrame, and the population effect size 'population_d' is known:\n",
    "identify_and_save_subsamples(effect_sizes_df, generated_data, 0.4889332, 'Personality', 'Weekly_Hours', 'subsample')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
